---
title: "OneR"
abstract: "This document shows the process of training a OneR model for preclinical stroke triage. It covers data preprocessing steps, including handling missing values via bagged tree imputation and scaling. The document compares the performance of the OneR model against the 4-Item Stroke Scale (4I-SS) on a held-out test set. Evaluation metrics, including ROC and PR curves, along with confusion matrices and confidence intervals obtained through bootstrapping, are presented to assess model efficacy. Feature importance from the trained OneR model is also extracted and displayed."
editor: visual
---

## Load Packages

```{r}

# Project-relative paths & file management
library(here)

# Tidyverse and ML
library(tidyverse)
library(tidymodels)
library(caret)

# Preprocessing
library(themis) # Oversampling

# OneR
library(OneR)

# Evaluation
library(yardstick) 
library(PRROC) # calculate both the ROCAUC and PRAUC
library(pROC)  # plot and compaire the ROC Curves

# Sankey
library(ggalluvial)

```

**Load Data as a tibble**

```{r}

stroke_triage_data <- read_csv(here("Code",
                                    "Data Preprocessing",
                                    "data_manipulation_6_SFS",
                                    "data_manipulation_6_SFS.csv"))

```

**Set label as factor**

We convert the label as a factor, so the used algorithms know it is a classification task

```{r}

stroke_triage_data$EVT <- as.factor(stroke_triage_data$EVT)

```

# Preprocessing

## Set Data Types

Set data types for for robust imputation and modeling.

```{r}

stroke_triage_data <- stroke_triage_data %>% 
  mutate(
    EVT = factor(EVT, levels = c("1", "0")),

    # Ordinale Prädiktoren
    vigilance_4ISS           = factor(vigilance_4ISS,
                                      levels = c(0,1,2),
                                      ordered = TRUE),
    gaze_head_deviation_4ISS = factor(gaze_head_deviation_4ISS,
                                      levels = c(0,1,2),
                                      ordered = TRUE),
    hemiparesis_4ISS         = factor(hemiparesis_4ISS,
                                      levels = c(0,1,2),
                                      ordered = TRUE),

    # nominale Prädiktoren
    aphasia_dysarthria_4ISS  = factor(aphasia_dysarthria_4ISS),
    gender                   = factor(gender),
    medication_anticoags     = factor(medication_anticoags),
    exclusion_TOO            = factor(exclusion_TOO),
    pupil_reaction_abnormal  = factor(pupil_reaction_abnormal)
  )

```

## Train-Test Data Split

```{r}

# Set seed for reproducible and comparable Train-Test splits
set.seed(456)

# Split the dataset into training and testing sets
stroke_trage_data_split <- initial_split(stroke_triage_data,
                                         prop = 0.8,
                                         strata = EVT) 


# create seperate dataframes
train_data <- stroke_trage_data_split %>% training()
test_data <- stroke_trage_data_split %>% testing()

stroke_trage_data_split

```

## Imputation

Set Data Types for Imputation

```{r}

# Ensure correct data types for train data
train_data <- train_data %>%
  mutate(
    # Ordinal categorical variables (0,1,2)
    vigilance_4ISS = factor(vigilance_4ISS,
                            levels = c(0, 1, 2),
                            ordered = TRUE),
    gaze_head_deviation_4ISS = factor(gaze_head_deviation_4ISS,
                                      levels = c(0, 1, 2),
                                      ordered = TRUE),
    hemiparesis_4ISS = factor(hemiparesis_4ISS,
                              levels = c(0, 1, 2),
                              ordered = TRUE),

    # Categorical (non-ordinal) variables
    aphasia_dysarthria_4ISS = factor(aphasia_dysarthria_4ISS),
    gender = factor(gender),
    medication_anticoags = factor(medication_anticoags),
    exclusion_TOO = factor(exclusion_TOO),
    pupil_reaction_abnormal = factor(pupil_reaction_abnormal),

    # Ensure outcome variable is a factor
    EVT = factor(EVT, levels = c("1", "0")) 
  )

# Apply the same transformations to test data
test_data <- test_data %>%
  mutate(
    vigilance_4ISS = factor(vigilance_4ISS,
                            levels = c(0, 1, 2),
                            ordered = TRUE),
    gaze_head_deviation_4ISS = factor(gaze_head_deviation_4ISS,
                                      levels = c(0, 1, 2),
                                      ordered = TRUE),
    hemiparesis_4ISS = factor(hemiparesis_4ISS,
                              levels = c(0, 1, 2),
                              ordered = TRUE),

    aphasia_dysarthria_4ISS = factor(aphasia_dysarthria_4ISS),
    gender = factor(gender),
    medication_anticoags = factor(medication_anticoags),
    exclusion_TOO = factor(exclusion_TOO),
    pupil_reaction_abnormal = factor(pupil_reaction_abnormal),

    EVT = factor(EVT, levels = c("1", "0"))
  )

```

**Apply Bagged Tree Imputation**

We analyzed the `data_manipulation_5_imputation.qmd` file and found that `step_impute_bag()` provides the most suitable imputation method for our dataset.

Each feature with missing values is treated as the **target (label)**, while all other available features act as **predictors**. Multiple decision trees (like in Random Forest, but each tree uses all predictors) are trained on different random subsets of the data. The missing values are then predicted using these models, and this process is repeated separately for each variable with missing data.

```{r}

# Define the recipe for imputation using bagged trees
imputation_recipe <- recipe(EVT ~ ., data = train_data) %>%
  step_impute_bag(all_predictors())  

# Train the recipe using only the training data
trained_imputation_recipe <- imputation_recipe %>%
  prep(training = train_data, retain = TRUE)  

# Apply the trained recipe separately to training and test data
train_data <- bake(trained_imputation_recipe, new_data = train_data)  
test_data  <- bake(trained_imputation_recipe, new_data = test_data)  

```

Convert all Factors to Numeric

```{r}

train_data <- train_data %>%
  mutate(across(where(is.factor), ~ as.numeric(as.character(.))))  # Convert factors to numeric

test_data <- test_data %>%
  mutate(across(where(is.factor), ~ as.numeric(as.character(.))))  # Convert factors to numeric

```

## 4I-SS Reference

**Calculate the 4I-SS**

The sum of the four individual items of the 4I-SS is calculated after imputation, ensuring that a score is obtained for each observation. As imputation was performed separately for the training and test datasets to avoid data leakage, the 4I-SS is also calculated separately for each dataset.

```{r}

# Calculate 4I-SS for train data
train_data <- train_data %>%
  mutate(sum_4ISS = vigilance_4ISS +
                         gaze_head_deviation_4ISS +
                         hemiparesis_4ISS +
                         aphasia_dysarthria_4ISS)

# Calculate 4I-SS for test data
test_data <- test_data %>%
  mutate(sum_4ISS = vigilance_4ISS +
                         gaze_head_deviation_4ISS +
                         hemiparesis_4ISS +
                         aphasia_dysarthria_4ISS)

```

**Calculate 4I-SS EVT Decisions**

The ML model should outperform the 4-Item Stroke Scale (4I-SS). Therefore, the decisions made by the 4I-SS on the same test set need to be calculated and saved in a separate dataframe for comparison.

```{r}

classification_4ISS <- test_data %>% 
  mutate(classification_EVT = if_else(sum_4ISS > 2, 1, 0)) %>% 
  select(sum_4ISS, classification_EVT)

```

## Select Features Manually

**Get Current Features**

```{r}

colnames(stroke_triage_data)

```

**Calculate the 4I-SS**

The sum of the four individual items of the 4I-SS is calculated after imputation, ensuring that a score is obtained for each observation. As imputation was performed separately for the training and test datasets to avoid data leakage, the 4I-SS is also calculated separately for each dataset.

```{r}

# Calculate 4I-SS for train data
train_data <- train_data %>%
  mutate(sum_4ISS = vigilance_4ISS +
                         gaze_head_deviation_4ISS +
                         hemiparesis_4ISS +
                         aphasia_dysarthria_4ISS)

# Calculate 4I-SS for test data
test_data <- test_data %>%
  mutate(sum_4ISS = vigilance_4ISS +
                         gaze_head_deviation_4ISS +
                         hemiparesis_4ISS +
                         aphasia_dysarthria_4ISS)

```

**Select 14 Features from Feature Selection**

Optionally, the aggregated 4I-SS score can be used in place of the four individual items. Either the variables vigilance_4ISS, gaze_head_deviation_4ISS, hemiparesis_4ISS, and aphasia_dysarthria_4ISS or the total score fourISS_score should be commented out accordingly to avoid redundant information and multicollinearity.

```{r}

# for train data
train_data <- train_data %>% 
  select(gender,
         age,
         bp_systolic,
         pulse,
         blood_glucose,
         temperature,
         vigilance_4ISS,
         gaze_head_deviation_4ISS,
         hemiparesis_4ISS,
         aphasia_dysarthria_4ISS,
#         sum_4ISS,
         medication_anticoags,
         TOO,
         exclusion_TOO,
         pupil_reaction_abnormal,
         EVT)


# for test data
test_data <- test_data %>% 
  select(gender,
         age,
         bp_systolic,
         pulse,
         blood_glucose,
         temperature,
         vigilance_4ISS,
         gaze_head_deviation_4ISS,
         hemiparesis_4ISS,
         aphasia_dysarthria_4ISS,
#         sum_4ISS,
         medication_anticoags,
         TOO,
         exclusion_TOO,
         pupil_reaction_abnormal,
         EVT)

```

**Set EVT to a Factor**

```{r}

# train data
train_data$EVT <- factor(train_data$EVT)

# test data
test_data$EVT <- factor(test_data$EVT)

# 4I-SS data
classification_4ISS$classification_EVT  <- factor(classification_4ISS$classification_EVT)

```

Check the proportion of the label in training and test data to ensure they are close to each other (The proportion is equal for scaled and unscaled data).

```{r}

label_proportion_train <- table(train_data$EVT)

label_proportion_test <- table(test_data$EVT)

# Absolute Distribution
print(label_proportion_train)
print(label_proportion_test)

# Proportions
print(round(prop.table(label_proportion_train), digits = 2))
print(round(prop.table(label_proportion_test), digits = 2))

```

# OneR

**Prepaire Data**

As OneR decides the best rule based on achieved accuracy, it is highly affected by label class imbalance. We will first find a One Rule with the original data and then with the balanced dataset.

**Downsample data to balanced dataset**

```{r}

set.seed(456)

# Get all positive EVT observations of the training set
positive_samples <- train_data %>% filter(EVT == 1) 

# Sample 77 negative instances
negative_samples <- train_data %>% filter(EVT == 0) %>% sample_n(73)

# Combine the positive and negative samples into a balanced DataFrame
train_data_balanced <- bind_rows(positive_samples, negative_samples)

# Check the distribution of the classes
table(train_data_balanced$EVT)

```

## Train

### Unbalanced Training

```{r}

# Train the OneR model
one_r_model <- OneR(
  formula = EVT ~ .,      # Specify the formula. EVT is the outcome variable, and . indicates all other columns are predictors
  data = train_data,      # Provide the training dataset
  verbose = TRUE          # Print detailed output during the training process for better understanding
)

# Output the model
summary(one_r_model)

```

### Balanced Training

```{r}

# Train the OneR model
one_r_model_balanced <- OneR(
  formula = EVT ~ .,      # Specify the formula. EVT is the outcome variable, and . indicates all other columns are predictors
  data = train_data_balanced,      # Provide the training dataset
  verbose = TRUE          # Print detailed output during the training process for better understanding
)

# Output the model
summary(one_r_model_balanced)

```

# Predict

The One rule from OneR needs to be manually applied to the data, as `predict()` does not work with the model.

```{r}

# OneR_predictions_test <- predict(one_r_model_balanced, test_data) 
# 
# eval_model(prediction, test_data)

```

**One Rule**

```{r}

# predict with hemiparesis rule
  OneR_predictions_test <- as_factor(ifelse(test_data$hemiparesis_4ISS == 2, 1, 0))

```

# Evaluate - Single Seed

This is the evaluation of a single train–test split using one chosen seed. It provides an initial look at model performance and is mainly kept to support understanding of hyperparameter behavior (e.g., via `autoplot()`) and to explore overfitting and underfitting dynamics. In the next section, nested cross-validation is used to estimate more reliable and statistically sound performance metrics.

Since we only have 94 EVT cases and the train–test split is 80–20, the test set includes just around 18–20 EVT cases. With so few cases, even a handful of challenging, atypical, mislabeled, or poorly documented cases can strongly influence the evaluation results for a specific split and seed. This variability can lead to unstable and potentially misleading performance estimates.

While this section is informative for inspecting model behavior, performance metrics based on this single split should be interpreted with caution. The nested cross-validation in the next section provides a more robust estimate of predictive performance, serving a role comparable to confidence intervals by capturing variability across multiple resampled data splits.

## Confusion Matrix and Statistics

Evaluate the prediction with caret `confusionMatrix` function.

```{r}

cm_OneR_test <- confusionMatrix(OneR_predictions_test,
                test_data$EVT,
                positive = "1")

print(cm_OneR_test)

```

**Compared to 4I-SS**

```{r}

cm_4ISS_test <- confusionMatrix(data = classification_4ISS$classification_EVT,
                reference = test_data$EVT,
                positive = "1")

print(cm_4ISS_test)

```

## Confidence Intervals

**Preprocess Data**

Preprocess the data for the `calc_boot_metrics_function` function

```{r}

# convert to numerical data any mimic probabilities for ROC and AUCROC and AUCPR
probabilities_OneR <- as.numeric(as.character(OneR_predictions_test))

# Training
label_train <- train_data$EVT

# Testing
label_test <- test_data$EVT

# 4I-SS
label_4ISS <- test_data$EVT

# Prepare data frame for BCa bootstrap (test-set predictions)

OneR_test_data_bootstrapping <- tibble::tibble(
  pred_class = OneR_predictions_test,  # predicted class labels
  .pred_1    = probabilities_OneR,     # predicted probability for class "1"
  EVT        = label_test              # true outcome
) %>%
  dplyr::mutate(
    EVT        = factor(as.character(EVT),        levels = c("0", "1")),
    pred_class = factor(as.character(pred_class), levels = c("0", "1")),
    .pred_1    = as.numeric(.pred_1)
  )

```

**Function to calculate metrics for each Bootstrap**

We create a custom function to calculate all metrics for each bootstrapped dataset.

```{r}

calc_boot_metrics_function <- function(data, indices, ...) {
  
  # Create a new dataset by bootstrapping the original rows (patient-level resampling)
  data <- data[indices, ]
  
  # Define actual and predicted labels as factors with "0" = negative, "1" = positive
  actual    <- factor(data$EVT,        levels = c("0", "1"))
  predicted <- factor(data$pred_class, levels = c("0", "1"))
  
  # Extract predicted probability for the positive class ("1")
  probs <- data$.pred_1
  
  # Numeric truth vector for manual Brier calculation
  truth_num <- as.numeric(actual) - 1L
  
  # Build ROC curve object using pROC
  roc_obj <- pROC::roc(
    response  = actual,    # true labels
    predictor = probs,     # predicted probabilities for class "1"
    levels    = c("0", "1"),
    direction = "<"        # higher probs indicate class "1"
  )
  
  # Build Precision–Recall curve object using PRROC
  pr_obj <- PRROC::pr.curve(
    scores.class0 = probs[actual == "1"],  # probabilities for positive cases
    scores.class1 = probs[actual == "0"]   # probabilities for negative cases
  )
  
  # Return scalar performance metrics as a named numeric vector
  c(
    accuracy    = yardstick::accuracy_vec(actual, predicted),
    sensitivity = yardstick::sensitivity_vec(actual, predicted, event_level = "second"),
    specificity = yardstick::specificity_vec(actual, predicted, event_level = "second"),
    precision   = yardstick::precision_vec(actual, predicted, event_level = "second"),
    npv         = yardstick::npv_vec(actual, predicted, event_level = "second"),
    f1          = yardstick::f_meas_vec(actual, predicted, event_level = "second"),
    brier       = mean((truth_num - probs)^2, na.rm = TRUE),
    aucroc      = as.numeric(roc_obj$auc),
    aucpr       = as.numeric(pr_obj$auc.integral)
  )
}

```

**Create and Calculate Metrics for All Bootstraps**

The `boot::boot()` function generates R non-parametric bootstrap resamples (sampling test-set observations with replacement) from the original test dataset and applies the user-defined statistic function to each resample to recompute the performance metrics.

```{r}

set.seed(456)

# Run nonparametric BCa bootstrap using boot package

boot_results <- boot::boot(
  data      = OneR_test_data_bootstrapping,   # dataset to bootstrapp
  statistic = calc_boot_metrics_function,     # custom function to calcualte all metrics
  R         = 5000                            # amount of bootstrapps
)

# Extract metric names (and their order) from the original statistic
# to consistently label bootstrap replicates and confidence intervals

metric_names <- names(boot_results$t0)

```

**Visual Bootstrap Distribution Diagnostics for Confidence Interval Selection**

We plot the distributions to assess whether the metric values from the 5,000 bootstrap resamples are approximately symmetric or noticeably skewed. Based on this visual check, we decide whether to report percentile confidence intervals or to use bias-corrected and accelerated (BCa) confidence intervals computed from the 5,000 bootstrap values.

```{r}

# Convert bootstrap replicate matrix to a tidy long data frame
# For the ggridges package

boot_results_long <- as.data.frame(boot_results$t) %>%
  setNames(metric_names) %>%
  tidyr::pivot_longer(
    cols      = dplyr::everything(),
    names_to  = "metric",
    values_to = "value"
  ) %>%
  dplyr::mutate(
    metric = factor(metric, levels = metric_names)
  )


# Ridgeline plot of bootstrap distributions (using ggridges)

ggplot(boot_results_long, aes(x = value, y = metric, fill = metric)) +
  ggridges::geom_density_ridges(
    alpha = 0.7,
    scale = 1.1,
    color = "black",
    linewidth = 0.4
  ) +
  theme_ridges() +
  theme(legend.position = "none") +
  labs(x = NULL, y = NULL)

```

**Determine Bias-Corrected Confidence Intervals (BCa)**

Because the bootstrap distribution plots showed skewness for some performance metrics, we calculated bias-corrected and accelerated (BCa) confidence intervals using `boot::boot.ci(type = "bca")`. BCa intervals still provide a 95% confidence interval, but unlike the percentile method they do not have to use 2.5% in each tail. Instead, BCa shifts the lower and upper cut-offs to account for bias and skewness in the bootstrap distribution, while the total probability outside the interval remains 5%.

```{r}

# Compute BCa 95% confidence intervals for each metric

metrics_and_CI_bca <- purrr::map_dfr(seq_along(metric_names), function(i) {

  # Compute 95% BCa CI for metric i
  ci <- boot::boot.ci(
    boot_results,
    type  = "bca",   # bias-corrected and accelerated (BCa) method
    conf  = 0.95,    # 95% confidence level
    index = i
  )

  # Bootstrap mean (mean over all resamples) for metric i
  boot_mean <- mean(boot_results$t[, i], na.rm = TRUE)

  tibble::tibble(
    metric = metric_names[i],
    value  = boot_mean,
    lower  = ci$bca[4],
    upper  = ci$bca[5]
  )
}) %>%
  dplyr::mutate(
    CI_95 = sprintf("(%.3f - %.3f)", lower, upper)
  ) %>%
  dplyr::select(metric, value, CI_95)

# Print metrics and CIs
print(metrics_and_CI_bca)

```

## ROC and PROC

```{r}

# convert to numerical data any mimic probabilities
probabilities_OneR <- as.numeric(as.character(OneR_predictions_test))

# Training
label_train <- train_data$EVT

# Testing
label_test <- test_data$EVT

# 4I-SS
label_4ISS <- test_data$EVT

# 4I-SS score 
score_4ISS <- classification_4ISS$sum_4ISS

```

```{r}

# Configure the plot region to be square
par(pty = "s")

# ROC for the training model
ROC_Train <- roc(label_test, probabilities_OneR ,
    plot = TRUE,
    legacy.axes = TRUE,
    percent = TRUE,
    xlab = "1 - Specificity in %",
    ylab = "Sensitivity in %",
    col = "#6F99ADE6",
    lwd = 4,
    main = "ROC Curve Comparison",
    levels = c(0, 1),         # Set 0 as control (no EVT) and 1 as case (EVT)
    direction = "<"           # Direction: control < case (0 < 1)
)


# ROC for the 4I-SS model
ROC_4ISS <- plot.roc(label_4ISS, score_4ISS,
    add = TRUE,  # Add to the existing plot
    percent = TRUE,
    col = "darkgrey",
    lwd = 4,
    levels = c(0, 1),      
    direction = "<"          
)

# Statistical comparison between ROC curves of training and 4I-SS models
ROC_COMBINE <- roc.test(ROC_Train, ROC_4ISS)

# Plot adjustments: add p-value and legend
text(50, 50, labels = paste("p-value =", format.pval(ROC_COMBINE$p.value)), adj = c(0, .5))
legend("bottomright", legend = c("Test", "4I-SS"), col = c("#6F99ADE6", "darkgrey"), lwd = 4)

```

## Sankey Plot

In this section, we generate a Sankey diagram to visualize the data flow starting from the test set. The diagram traces the path from the machine learning model's predictions to the true labels, ultimately categorizing the results into the confusion matrix metrics (True Positives, True Negatives, False Positives, False Negatives). This visualization offers a more intuitive comparison of proportions than standard tabular confusion matrices.

**Prepare the Data**

```{r}

# 1. Combine vectors into a dataframe
cm_df_OneR <- data.frame(
  pred_class = OneR_predictions_test, 
  EVT        = label_test
)

# 2. Calculate Confusion Matrix types
conf_matrix_counts <- cm_df_OneR %>%
  mutate(
    type = case_when(
      as.character(EVT) == "0" & as.character(pred_class) == "0" ~ "TN",
      as.character(EVT) == "0" & as.character(pred_class) == "1" ~ "FP",
      as.character(EVT) == "1" & as.character(pred_class) == "0" ~ "FN",
      as.character(EVT) == "1" & as.character(pred_class) == "1" ~ "TP"
    )
  ) %>%
  count(type) %>%
  pull(n, name = type)

# 3. Extract values (safely handling zeros if a case doesn't exist)
TN <- ifelse(is.na(conf_matrix_counts["TN"]), 0, conf_matrix_counts["TN"])
FP <- ifelse(is.na(conf_matrix_counts["FP"]), 0, conf_matrix_counts["FP"])
FN <- ifelse(is.na(conf_matrix_counts["FN"]), 0, conf_matrix_counts["FN"])
TP <- ifelse(is.na(conf_matrix_counts["TP"]), 0, conf_matrix_counts["TP"])

# 4. Calculate Flow Totals
count_test_data <- nrow(test_data)

# Predictions (Model Output)
model_positive <- TP + FP 
model_negative <- TN + FN

# Actual Labels (Ground Truth)
label_positive <- TP + FN 
label_negative <- TN + FP

```

**Define Nodes**

Defines the nodes layout without the streams.

Every instance is a single vertical step.

```{r}

# Define nodes with names
# The row order determines the ID (0-indexed for networkD3)

nodes <- tibble(

  node_name_and_count = c(
    # define the names of the labels and their counts
    sprintf("Test Set (n = %d)",          count_test_data),  # ID 0: Instance 1
    
    sprintf("Prediction No EVT (n = %d)", model_negative), # ID 1: Instance 2 (Top)
    sprintf("Prediction EVT (n = %d)",    model_positive), # ID 2: Instance 2 (Bottom)
    
    sprintf("EVT Not Conducted (n = %d)", label_negative), # ID 3: Instance 3 (Top)
    sprintf("EVT Conducted (n = %d)",     label_positive), # ID 4: Instance 3 (Bottom)
    
    sprintf("True Negative (n = %d)",     TN),             # ID 5: Instance 4 (TN)
    sprintf("False Positive (n = %d)",    FP),             # ID 6: Instance 4 (FP)
    sprintf("False Negative (n = %d)",    FN),             # ID 7: Instance 4 (FN)
    sprintf("True Positive (n = %d)",     TP)              # ID 8: Instance 4 (TP)
  ),
  node_color = c(
    "Node_Test",
    "Node_Pred",
    "Node_Pred",
    "Node_Label",
    "Node_Label",
    "GoodNode",
    "BadNode",
    "BadNode",
    "GoodNode"
  )
)

nodes <- as.data.frame(nodes)

```

**Define Links**

Defines the flow logic (streams) of the data and the groups for the colors (each group gets a different color)

```{r}

links <- data.frame(
  # Index of source node
  source = c(0, 0, 1, 1, 2, 2, 3, 3, 4, 4), 
  
  # Index of target node
  target = c(1, 2, 3, 4, 3, 4, 5, 6, 7, 8),
  
  # Magnitude of each flow
  value  = c(TN + FN, TP + FP, TN, FN, FP, TP, TN, FP, FN, TP),
  
  # Group for coloring links
  # 0 -> 1/2       : Test Set -> Prediction      = FlowTestPred
  # 1/2 -> 3/4     : Prediction -> Label         = FlowPredLabel
  # 3/4 -> 5/6/7/8 : Label -> Outcome (TN/FP/FN/TP) = FlowOutcome (neutral)
  flow_color  = c(
    "FlowTestPred", "FlowTestPred",   # 0 → 1,2
    "FlowPredLabel","FlowPredLabel",  # 1 → 3,4
    "FlowPredLabel","FlowPredLabel",  # 2 → 3,4
    "GoodNode",  # 3 → 5 (TN)  green
    "BadNode",   # 3 → 6 (FP)  red
    "BadNode",   # 4 → 7 (FN)  red
    "GoodNode"   # 4 → 8 (TP)  green
  )
)

```

**Define Colors**

```{r}

colourScale <- '
d3.scaleOrdinal()
  .domain([
    "FlowTestPred",
    "FlowPredLabel",
    "FlowOutcome",
    "Node_Test",
    "Node_Pred",
    "Node_Label",
    "GoodNode",
    "BadNode"
  ])
  .range([
    "#E18727E6",    // Test -> Pred
    "#E18727E6",    // Pred -> Label
    "#D0AF84FF",    // Label -> Outcome (neutral)
    "#E18727E6",    // Test Data
    "#E18727E6",    // Predictions
    "#E18727E6",    // Label
    "#20854EE6",    // GoodNode (TN/TP) – green
    "#BC3C29E6"     // BadNode  (FP/FN) – red
  ])
'

```

**Plot the Sankey**

We use `networkD3::sankeyNetwork` to create an interactive Sankey diagram in HTML format.

```{r}

networkD3::sankeyNetwork(
  Links      = links,
  Nodes      = nodes,
  Source     = "source",
  Target     = "target",
  Value      = "value",
  NodeID     = "node_name_and_count",
  LinkGroup  = "flow_color",      
  NodeGroup  = "node_color",
  colourScale = colourScale,
  nodeWidth  = 10,
  nodePadding = 30,
  fontSize   = 14,
  iterations = 0
  )

```

# Save the Model

```{r}

# Save the trained model
saveRDS(one_r_model_balanced, here("Code", "Machine Learning", "models", "final_OneR_model.rds"))

```
