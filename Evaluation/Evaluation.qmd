---
title: "Evaluation"
editor: visual
---

### Load Packages

```{r}

# for rendering pdf files
library(showtext) 

# Project-relative paths & file management
library(here)

library(tidyverse)
library(tidymodels)

# Evaluation
library(yardstick) 
library(PRROC) # calculate both the ROCAUC and PRAUC
library(pROC)  # plot and compare the (Ensemble) ROC Curves

```

### Load Data

```{r}

stroke_triage_data <- read_csv(here("Code",
                                    "Data Preprocessing",
                                    "data_manipulation_6_SFS",
                                    "data_manipulation_6_SFS.csv"))

```

### Load Models

```{r}

# Load each model

final_OneR_model <- readRDS(here("Code",
                                 "Machine Learning",
                                 "models",
                                 "final_OneR_model.rds"))

final_knn_model  <- readRDS(here("Code",
                                 "Machine Learning",
                                 "models",
                                 "final_knn_model.rds"))

final_cart_model <- readRDS(here("Code",
                                 "Machine Learning",
                                 "models",
                                 "final_cart_model.rds"))

final_xgb_model  <- readRDS(here("Code",
                                 "Machine Learning",
                                 "models",
                                 "final_xgb_model.rds"))

final_rf_model   <- readRDS(here("Code",
                                 "Machine Learning",
                                 "models",
                                 "final_rf_model.rds"))

final_nn_model   <- readRDS(here("Code",
                                 "Machine Learning",
                                 "models",
                                 "final_nn_model.rds"))

```

# Preprocessing

## Train-Test Data Split

```{r}

# Set seed for reproducible and comparable Train-Test split
set.seed(456)

# Split the dataset into training and testing sets
stroke_triage_data_split <- initial_split(stroke_triage_data,
                                         prop = 0.8,
                                         strata = EVT) 

# create separate dataframes
train_data <- stroke_triage_data_split %>% training()
test_data <- stroke_triage_data_split %>% testing()

stroke_triage_data_split

```

## Imputation

Set Data Types for Imputation

```{r}

# Ensure correct data types for train data
train_data <- train_data %>%
  mutate(vigilance_4ISS  = factor(vigilance_4ISS,
                                  levels = c(0, 1, 2),
                                  ordered = TRUE),
gaze_head_deviation_4ISS = factor(gaze_head_deviation_4ISS,
                                  levels = c(0, 1, 2),
                                  ordered = TRUE),
        hemiparesis_4ISS = factor(hemiparesis_4ISS,
                                  levels = c(0, 1, 2),
                                  ordered = TRUE),

    # Categorical (non-ordinal) variables
    aphasia_dysarthria_4ISS = factor(aphasia_dysarthria_4ISS),
    gender = factor(gender),
    medication_anticoags = factor(medication_anticoags),
    exclusion_TOO = factor(exclusion_TOO),
    pupil_reaction_abnormal = factor(pupil_reaction_abnormal),

    # Ensure outcome variable is a factor
    EVT = factor(EVT, levels = c("0", "1"))  
  )

# Apply the same transformations to test data
test_data <- test_data %>%
  mutate(
    vigilance_4ISS = factor(vigilance_4ISS,
                            levels = c(0, 1, 2),
                            ordered = TRUE),
    gaze_head_deviation_4ISS = factor(gaze_head_deviation_4ISS,
                                      levels = c(0, 1, 2),
                                      ordered = TRUE),
    hemiparesis_4ISS = factor(hemiparesis_4ISS,
                              levels = c(0, 1, 2),
                              ordered = TRUE),

    aphasia_dysarthria_4ISS = factor(aphasia_dysarthria_4ISS),
    gender = factor(gender),
    medication_anticoags = factor(medication_anticoags),
    exclusion_TOO = factor(exclusion_TOO),
    pupil_reaction_abnormal = factor(pupil_reaction_abnormal),

    EVT = factor(EVT, levels = c("0", "1"))
  )

```

**Apply Bagged Tree Imputation**

We analyzed the `data_manipulation_5_imputation.qmd` file and found that `step_impute_bag()` provides the most suitable imputation method for our dataset.

Each feature with missing values is treated as the **target (label)**, while all other available features act as **predictors**. Multiple decision trees (like in Random Forest, but each tree uses all predictors) are trained on different random subsets of the data. The missing values are then predicted using these models, and this process is repeated separately for each variable with missing data.

```{r}

# Define the recipe for imputation using bagged trees
imputation_recipe <- recipe(EVT ~ ., data = train_data) %>%
  step_impute_bag(all_predictors())  

# Train the recipe using only the training data
trained_imputation_recipe <- imputation_recipe %>%
  prep(training = train_data, retain = TRUE)  

# Apply the trained recipe separately to training and test data
train_data <- bake(trained_imputation_recipe, new_data = train_data)  
test_data  <- bake(trained_imputation_recipe, new_data = test_data)  

```

Convert all Factors to Numeric

```{r}
#| warning: false

train_data <- train_data %>%
  mutate(across(where(is.factor), ~ as.numeric(as.character(.))), # Convert factors to numeric
         EVT = factor(EVT, levels = c("1", "0")))                 # The EVT label as a factor

test_data <- test_data %>%
  mutate(across(where(is.factor), ~ as.numeric(as.character(.))), # Convert factors to numeric
         EVT = factor(EVT, levels = c("1", "0")))                 # The EVT label as a factor

```

## Select Features Manually

**Get Current Features**

```{r}

colnames(stroke_triage_data)

```

**Select 14 Features from Feature Selection**

```{r}

# for train data
train_data <- train_data %>% 
  select(gender,
         age,
         bp_systolic,
         pulse,
         blood_glucose,
         temperature,
         vigilance_4ISS,
         gaze_head_deviation_4ISS,
         hemiparesis_4ISS,
         aphasia_dysarthria_4ISS,
         medication_anticoags,
         TOO,
         exclusion_TOO,
         pupil_reaction_abnormal,
         EVT)


# for test data
test_data <- test_data %>% 
  select(gender,
         age,
         bp_systolic,
         pulse,
         blood_glucose,
         temperature,
         vigilance_4ISS,
         gaze_head_deviation_4ISS,
         hemiparesis_4ISS,
         aphasia_dysarthria_4ISS,
         medication_anticoags,
         TOO,
         exclusion_TOO,
         pupil_reaction_abnormal,
         EVT)

```

## Preprocessing

### Set Data Type for CART and Random Forest

**CART**

```{r}

# Set column classes in test data
test_data_cart <- test_data %>% 
  mutate(
    vigilance_4ISS           = factor(vigilance_4ISS),
    gaze_head_deviation_4ISS = factor(gaze_head_deviation_4ISS),
    hemiparesis_4ISS         = factor(hemiparesis_4ISS),

    # Categorical (non-ordinal) variables
    aphasia_dysarthria_4ISS = factor(aphasia_dysarthria_4ISS),
    gender = factor(gender),
    medication_anticoags = factor(medication_anticoags),
    exclusion_TOO = factor(exclusion_TOO),
    pupil_reaction_abnormal = factor(pupil_reaction_abnormal),

    # Ensure outcome variable is a factor
    EVT = factor(EVT, levels = c("1", "0"))
  )

```

**Random Forrest**

```{r}

test_data_rf <- test_data %>% 
  mutate(
    vigilance_4ISS           = factor(vigilance_4ISS),
    gaze_head_deviation_4ISS = factor(gaze_head_deviation_4ISS),
    hemiparesis_4ISS         = factor(hemiparesis_4ISS),

    # Categorical (non-ordinal) variables
    aphasia_dysarthria_4ISS = factor(aphasia_dysarthria_4ISS),
    gender = factor(gender),
    medication_anticoags = factor(medication_anticoags),
    exclusion_TOO = factor(exclusion_TOO),
    pupil_reaction_abnormal = factor(pupil_reaction_abnormal),

    # Ensure outcome variable is a factor
    EVT = factor(EVT, levels = c("1", "0"))
  )

```

### Create 4I-SS Reference

The ML model should outperform the 4-Item Stroke Scale (4I-SS). Therefore, the decisions made by the 4I-SS on the same test set need to be calculated and saved in a separate dataframe for comparison.

```{r}

classification_4ISS <- test_data %>% 
  mutate(Score_4ISS = hemiparesis_4ISS +
           vigilance_4ISS +
           gaze_head_deviation_4ISS +
           aphasia_dysarthria_4ISS,
         classification_EVT = as_factor(if_else(Score_4ISS > 2, 1, 0))) %>% 
  select(Score_4ISS, classification_EVT)

```

### Predictions

**OneR**

The One rule from OneR needs to be manually applied to the data, as `predict()` does not work with the model.

```{r}

# predict with hemiparesis rule
OneR_predictions_test <- as_factor(ifelse(test_data$hemiparesis_4ISS == 2, 1, 0))

# extract test label
test_label <- as_factor(test_data$EVT)

# convert to numerical data to mimic probablities for ROC
probabilities_OneR <- as.numeric(as.character(OneR_predictions_test))

```

**4I-SS**

```{r}

# extract score
score_4ISS <- classification_4ISS$Score_4ISS

# min/max scaling
probabilities_score_4ISS <- (score_4ISS - min(score_4ISS)) / (max(score_4ISS) - min(score_4ISS))

# now the values of the 4I-SS can be used as probabilities
probabilities_score_4ISS

```

**Other Models**

```{r}

probabilities_knn <- predict(final_knn_model,
                             new_data = test_data,
                             type = "prob")$.pred_1

probabilities_cart <- predict(final_cart_model,
                              new_data = test_data_cart,
                              type = "prob")$.pred_1

probabilities_xgb <- predict(final_xgb_model,
                             new_data = test_data,
                             type = "prob")$.pred_1

probabilities_rf <- predict(final_rf_model,
                            new_data = test_data_rf,
                            type = "prob")$.pred_1

probabilities_nn <- predict(final_nn_model,
                            new_data = test_data,
                            type = "prob")$.pred_1

```

### Get the label

```{r}

# Actual labels for comparison 
label_test <- test_data$EVT

```

### Get 4ISS Scores from Testdata

```{r}

# 4I-SS scores of test data
score_4ISS <- classification_4ISS$Score_4ISS

```

## ROC and PROC

**Used the same** color palette **for both plots**

```{r}

# Use the colors of the NEJM color palette
# We used alpha 0.9 as a filter, that the colors are not as bright

colors_nejm_alpha90 <- c(
  "#BC3C29E6",
  "#0072B5E6",
  "#E18727E6",
  "#20854EE6",
  "#7876B1E6",
  "#6F99ADE6",
  "#FFDC91E6",
  "#EE4C97E6"
)

```

**Mapping**

```{r}

# Map each model's scores to its name in `model_scores` for easy iteration.
labels <- label_test
model_scores <- list(
  `OneR`  = probabilities_OneR,
  `CART`  = probabilities_cart,
  `KNN`   = probabilities_knn,
  `XGB`   = probabilities_xgb,
  `RF`    = probabilities_rf,
  `NN`    = probabilities_nn,
  `4I-SS` = probabilities_score_4ISS
)

```

### ROC Curve

```{r}

# Configure plotting layout and text scaling
par(mfrow = c(1, 1),  # single panel
    pty = "s",        # square plotting region
    cex = 1.2)        # global text size scaling

# Initialize empty plot area for ROC curves
plot(NULL, xlim = c(0, 100), ylim = c(0, 100), 
     xlab = "1 - Specificity (%)", 
     ylab = "Sensitivity (%)", 
     main = "")

# Loop through model scores and plot ROC curves
for (i in seq_along(model_scores)) {
  roc_curve <- roc.curve(
    scores.class0 = model_scores[[i]][labels == 1], 
    scores.class1 = model_scores[[i]][labels == 0], 
    curve = TRUE
  )
  
  lines(roc_curve$curve[, 1] * 100,  # X: 1 - specificity (%)
        roc_curve$curve[, 2] * 100,  # Y: sensitivity (%)
        col = colors_nejm_alpha90[i],
        lwd = 3)
}

# diagonal line
abline(0, 1, lty = 2, col = "black", lwd = 2)

# Add legend to distinguish models
legend("bottomright",
       legend = names(model_scores),
       col = colors_nejm_alpha90,
       lwd = 3,
       cex = 0.8,
       bty = "n")

```

### Precision-Recall Curve

```{r}

# Configure plot layout and text scaling
par(mfrow = c(1, 1),  # single plot panel
    pty = "s",        # square plotting region
    cex = 1.0)        # base text size

# Initialize empty plot area for PR curves (axes only)
plot(NULL, xlim = c(0, 100), ylim = c(0, 100), 
     xlab = "Recall (%)", 
     ylab = "Precision (%)", 
     main = "")

# Loop through model scores and draw each PR curve
for (i in seq_along(model_scores)) {
  pr_curve <- pr.curve(
    scores.class0 = model_scores[[i]][labels == 1], 
    scores.class1 = model_scores[[i]][labels == 0], 
    curve = TRUE
  )
  
  # Plot precision-recall curve, scaled to percentages
  lines(pr_curve$curve[, 1] * 100,  # recall (%)
        pr_curve$curve[, 2] * 100,  # precision (%)
        col = colors_nejm_alpha90[i],
        lwd = 3)
}

# prevalence line
prevalence <- mean(labels == 1) * 100
abline(h = prevalence, lty = 2, col = "black", lwd = 2)

# Add legend identifying each model curve
legend("topright",
       legend = names(model_scores),
       col = colors_nejm_alpha90,
       lwd = 3,
       cex = 0.8,
       bty = "n")

```

### Combined Presentation

```{r}

# Plot configurations for 1x2 layout
par(mfrow = c(1, 2),
    pty = "s",
    cex = 1.2)

# ROC curves
plot(NULL, xlim = c(0, 100), ylim = c(0, 100), 
     xlab = "1 - Specificity (%)", 
     ylab = "Sensitivity (%)")

for (i in seq_along(model_scores)) {
  roc_curve <- roc.curve(
    scores.class0 = model_scores[[i]][labels == 1], 
    scores.class1 = model_scores[[i]][labels == 0], 
    curve = TRUE
  )
  lines(roc_curve$curve[, 1] * 100, 
        roc_curve$curve[, 2] * 100, 
        col = colors_nejm_alpha90[i], lwd = 3)
}

# Add diagonal random line (black, dashed)
abline(0, 1, lty = 2, col = "black", lwd = 2)

legend("bottomright",
       legend = names(model_scores),
       col = colors_nejm_alpha90,
       lwd = 4,
       seg.len = 0.6,
       x.intersp = 0.2,
       cex = 1,
       bty = "n",
       inset = c(0.03, 0))

# PR curves
plot(NULL, xlim = c(0, 100), ylim = c(0, 100), 
     xlab = "Recall (%)", 
     ylab = "Precision (%)")

for (i in seq_along(model_scores)) {
  pr_curve <- pr.curve(
    scores.class0 = model_scores[[i]][labels == 1], 
    scores.class1 = model_scores[[i]][labels == 0], 
    curve = TRUE
  )
  lines(pr_curve$curve[, 1] * 100, 
        pr_curve$curve[, 2] * 100, 
        col = colors_nejm_alpha90[i], lwd = 3)
}

# Add prevalence baseline (black, dashed)
prevalence <- mean(labels == 1) * 100
abline(h = prevalence, lty = 2, col = "black", lwd = 2)

legend("topright",
       legend = names(model_scores),
       col = colors_nejm_alpha90,
       lwd = 4,
       seg.len = 0.6,
       x.intersp = 0.2,
       cex = 1,
       bty = "n",
       inset = c(0.03, 0))

```

```{r}

# Plot configurations for 1x2 layout
par(mfrow = c(1, 2),
    pty = "s",
    cex = 1.2)

# ROC curves
plot(NULL, xlim = c(0, 100), ylim = c(0, 100), 
     xlab = "1 - Specificity (%)", 
     ylab = "Sensitivity (%)")

for (i in seq_along(model_scores)) {
  roc_curve <- roc.curve(
    scores.class0 = model_scores[[i]][labels == 1], 
    scores.class1 = model_scores[[i]][labels == 0], 
    curve = TRUE
  )
  lines(roc_curve$curve[, 1] * 100, 
        roc_curve$curve[, 2] * 100, 
        col = colors_nejm_alpha90[i], lwd = 3)
}

# Add diagonal random line
abline(0, 1, lty = 2, col = "black", lwd = 2)

legend("bottomright",
       legend = names(model_scores),
       col = colors_nejm_alpha90,
       lwd = 4,
       seg.len = 0.6,
       x.intersp = 0.2,
       cex = 1,
       bty = "n",
       inset = c(0.03, 0))

# PR curves
plot(NULL, xlim = c(0, 100), ylim = c(0, 100), 
     xlab = "Recall (%)", 
     ylab = "Precision (%)")

for (i in seq_along(model_scores)) {
  pr_curve <- pr.curve(
    scores.class0 = model_scores[[i]][labels == 1], 
    scores.class1 = model_scores[[i]][labels == 0], 
    curve = TRUE
  )
  lines(pr_curve$curve[, 1] * 100, 
        pr_curve$curve[, 2] * 100, 
        col = colors_nejm_alpha90[i], lwd = 3)
}

# Add prevalence baseline
prevalence <- mean(labels == 1) * 100
abline(h = prevalence, lty = 2, col = "black", lwd = 2)

legend("topright",
       legend = names(model_scores),
       col = colors_nejm_alpha90,
       lwd = 4,
       seg.len = 0.6,
       x.intersp = 0.2,
       cex = 1,
       bty = "n",
       inset = c(0.03, 0))

dev.off()

```

## Area Under the Curves

```{r}

# Initialize a data frame to store AUC values
auc_table <- data.frame(Model = character(), ROC_AUC = numeric(), PR_AUC = numeric(), stringsAsFactors = FALSE)

# Calculate ROC AUC and PR AUC values for each model
for (model_name in names(model_scores)) {
  
  # ROC AUC
  roc_result <- roc(labels,
                    model_scores[[model_name]],
                    levels = c(0, 1),
                    direction = "<")
  
  roc_auc_value <- as.numeric(auc(roc_result))  
  
  # PR AUC
  pr_result <- pr.curve(scores.class0 = model_scores[[model_name]][labels == 1], 
                        scores.class1 = model_scores[[model_name]][labels == 0], 
                        curve = FALSE)
  pr_auc_value <- pr_result$auc.integral  # Extract PR AUC
  
  # Store the AUC values in the table
  auc_table <- auc_table %>%
    add_row(Model = model_name,
            ROC_AUC = roc_auc_value,
            PR_AUC = pr_auc_value)
}

# Display the AUC table
print(auc_table)

```

## Risk Group Analysis

**Create Datasets**

For OneR, kNN and Neural Network

```{r}

# Create a copy of train_data for risk analysis and age groups
train_data_risk <- train_data %>%
  mutate(
    age_group = case_when(
      age < 60 ~ "<60",
      age >= 60 & age <= 80 ~ "60-80",
      age > 80 ~ ">80"
    )
  )

```

**Adjust Datatypes**

Random Forrest

```{r}

train_data_risk_rf <- train_data_risk %>% mutate(
    gender = as.factor(gender),
    medication_anticoags = as.factor(medication_anticoags),
    exclusion_TOO = as.factor(exclusion_TOO),
    pupil_reaction_abnormal = as.factor(pupil_reaction_abnormal),
    aphasia_dysarthria_4ISS = as.factor(aphasia_dysarthria_4ISS),
    
    # Ordinal variables as ordered factors
    vigilance_4ISS = factor(vigilance_4ISS),
    gaze_head_deviation_4ISS = factor(gaze_head_deviation_4ISS),
    hemiparesis_4ISS = factor(hemiparesis_4ISS),
    EVT = factor(EVT, levels = c("1", "0"))  # Ensure correct factor levels
)

```

### **Generate Model Predictions**

Predictions are generated for each model on the test data.

The age column is removed to prevent any influence on classification, as it was not included in the training data.

```{r}

# Generate predictions for each model on the training data with risk groups

# OneR model predictions using a rule on the hemiparesis feature
# Here, a hemiparesis score of 2 is assigned a prediction of 1 (positive), otherwise 0 (negative)
OneR_predictions_train_risk <- factor(ifelse(train_data_risk$hemiparesis_4ISS == 2, 1, 0), levels = c(0, 1))

# Convert factor predictions to numeric probabilities
OneR_probabilities_risk <- as.numeric(as.character(OneR_predictions_train_risk))  

# Generate predictions for the KNN model on train_data_risk
# Exclude the age_group variable to ensure it doesn’t influence predictions
KNN_probabilities_risk <- predict(final_knn_model, 
                                  new_data = train_data_risk %>% select(-age_group), 
                                  type = "prob")$.pred_1  # Extract the predicted probability for class "1"


# Generate predictions for the ANN model on train_data_risk
XGB_probabilities_risk <- predict(final_xgb_model, 
                                  new_data = train_data_risk %>% select(-age_group), 
                                  type = "prob")$.pred_1  # Extract probability for class "1"

# Generate predictions for the Random Forest model on train_data_risk_trees
RF_probabilities_risk <- predict(final_rf_model, 
                                 new_data = train_data_risk_rf %>% select(-age_group), 
                                 type = "prob")$.pred_1  # Extract probability for class "1"

# Generate predictions for the ANN model on train_data_risk
ANN_probabilities_risk <- predict(final_nn_model, 
                                  new_data = train_data_risk %>% select(-age_group), 
                                  type = "prob")$.pred_1  # Extract probability for class "1"


# Assign row names to train_data_risk based on a unique identifier or row numbers
# This ensures consistency in indexing when assigning names to each probability prediction
rownames(train_data_risk) <- as.character(1:nrow(train_data_risk))

# Assign names to each model’s probability output using the row names in train_data_risk
# This maintains alignment between the data and model predictions
names(OneR_probabilities_risk) <- rownames(train_data_risk)
names(KNN_probabilities_risk) <- rownames(train_data_risk)
names(XGB_probabilities_risk) <- rownames(train_data_risk)
names(RF_probabilities_risk) <- rownames(train_data_risk)
names(ANN_probabilities_risk) <- rownames(train_data_risk)

# Combine all model probability predictions into a single list
# This structure allows for easy access to each model's predictions when calculating ensemble scores or ROC curves
model_scores_risk <- list(
  OneR = OneR_probabilities_risk,
  KNN = KNN_probabilities_risk,
  XGB = XGB_probabilities_risk,
  RF = RF_probabilities_risk,
  ANN = ANN_probabilities_risk
)

```

**Define Group Lists**

Groups for Gender and Age are defined, and colors are assigned to each for plotting.

```{r}

# Split the training data into risk groups by Gender and Age, preserving row names
# This allows us to easily subset the data for each group and keep track of individual rows

risk_groups <- list(
  Gender = split(train_data_risk, train_data_risk$gender),  # Split data into "0" and "1" 
  Age = split(train_data_risk, train_data_risk$age_group)   # Split data into age groups
)

# Set labels

age_group_labels <- c("< 60", "60 - 80", "> 80")  # Age group labels

gender_labels <- c("Female", "Male")  # Gender labels for plotting and interpretation

```

**Verify Group Sizes**

If groups are too small ROC Curves can be missleading

```{r}

# Check subgroup sizes
for (i in 1:length(risk_groups$Age)) {
  subgroup <- risk_groups$Age[[i]]
  group_size <- nrow(subgroup)
  print(paste("Age Group", age_group_labels[i], "Size:", group_size))
  
  # Check class distribution
  EVT_labels_age <- factor(subgroup$EVT, levels = c(0, 1))
  class_counts <- table(EVT_labels_age)
  print(paste("Class distribution for Age Group", age_group_labels[i], ":"))
  print(class_counts)
}

```

**Aggregate the Predictions of all Models**

```{r}

# Convert the list of model predictions into a data frame
model_scores_df <- as.data.frame(model_scores_risk)

# Compute the ensemble prediction by averaging the predictions across models
ensemble_predictions <- rowMeans(model_scores_df, na.rm = TRUE)

```

**More Preprocessing**

```{r}

# Extract actual labels from the dataset, ensuring they are in the correct factor format
EVT_labels <- factor(train_data_risk$EVT, levels = c(0, 1))
names(EVT_labels) <- rownames(train_data_risk)  # Assign names to ensure alignment by row

# Verify that the names in EVT_labels match the names in ensemble_predictions
all_names_match <- all(names(EVT_labels) == names(ensemble_predictions))

# Align EVT_labels and ensemble_predictions by matching names if they do not match
if (!all_names_match) {
  common_indices <- intersect(names(EVT_labels), names(ensemble_predictions))  # Find common indices
  EVT_labels <- EVT_labels[common_indices]  # Subset labels to common indices
  ensemble_predictions <- ensemble_predictions[common_indices]  # Subset predictions to common indices
}

# Remove any entries with NA values to ensure clean data for ROC analysis
na_indices <- which(is.na(EVT_labels) | is.na(ensemble_predictions))
if (length(na_indices) > 0) {
  EVT_labels <- EVT_labels[-na_indices]  # Remove NA values from EVT_labels
  ensemble_predictions <- ensemble_predictions[-na_indices]  # Remove NA values from ensemble_predictions
}

```

## **Ensemble Stratified ROC Curves and AUC Values**

**Used Same Colors for Both Plots**

```{r}

gender_colors_risk <- colors_nejm_alpha90[6:7]  
age_colors_risk <- colors_nejm_alpha90[3:5]  

```

### ROC Gender

```{r}

# Plot settings
par(mfrow = c(1, 1),    # single plot
    pty = "s",          # square plotting region
    cex = 1.0)          # base font scaling

# Gender groups and AUC storage
gender_groups <- c("Female ", "Male")  
gender_aucs <- c()  # Empty vector to store AUC values for gender

plot_initialized <- FALSE

# Loop over each gender group, plot ROC, calculate AUC
for (i in 1:length(gender_groups)) {
  gender_code <- as.character(i - 1)  
  indices <- which(train_data_risk$gender == gender_code)
  EVT_labels_subgroup <- EVT_labels[indices]               
  ensemble_predictions_subgroup <- ensemble_predictions[indices]

  if (length(unique(EVT_labels_subgroup)) < 2) next

  ensemble_roc_subgroup <- roc(
    EVT_labels_subgroup,
    ensemble_predictions_subgroup,
    levels = c(0, 1),
    direction = "<",
    percent = TRUE
  )

  gender_aucs[i] <- round(ensemble_roc_subgroup$auc, 2)

  if (!plot_initialized) {
    plot(ensemble_roc_subgroup,
         col = gender_colors_risk[i], lwd = 4,
         legacy.axes = TRUE,
         xlab = "1 - Specificity in %",
         ylab = "Sensitivity in %")
    plot_initialized <- TRUE
  } else {
    plot(ensemble_roc_subgroup,
         col = gender_colors_risk[i], lwd = 4,
         legacy.axes = TRUE, add = TRUE)
  }
}

# Create legend text with AUC values
gender_labels_with_auc <- paste(gender_groups, "(AUC:", gender_aucs, ")")

# Add legend
legend("bottomright",
       legend = gender_labels_with_auc,
       col = gender_colors_risk,
       lwd = 4,
       cex = 0.8,
       bty = "n",
       inset = c(-0.1, 0))

```

### ROC Age Groups

```{r}

# Define colors for age groups using the Acadia palette
age_colors_risk <- colors_nejm_alpha90[3:5]  # Next three colors for age groups

# Plot configuration
par(mfrow = c(1, 1),  # single plot panel
    pty = "s",        # square plot region
    cex = 1.0)        # global text scaling

# Age Group ROC Curves 
age_groups <- unique(train_data_risk$age_group)
age_group_labels <- c("< 60    ", "60 - 80", "> 80    ")
age_aucs <- c()  # Empty vector to store AUC values

plot_initialized <- FALSE

# Loop through age groups, plot ROC, calculate AUC
for (i in seq_along(age_groups)) {
  age_group <- age_groups[i]
  indices <- which(train_data_risk$age_group == age_group)
  EVT_labels_subgroup <- EVT_labels[indices]                
  ensemble_predictions_subgroup <- ensemble_predictions[indices]

  if (length(unique(EVT_labels_subgroup)) < 2) next

  ensemble_roc_subgroup <- roc(
    EVT_labels_subgroup,
    ensemble_predictions_subgroup,
    levels = c(0, 1),
    direction = "<",
    percent = TRUE
  )

  age_aucs[i] <- round(ensemble_roc_subgroup$auc, 2)

  if (!plot_initialized) {
    plot(ensemble_roc_subgroup,
         col = age_colors_risk[i], lwd = 4,
         legacy.axes = TRUE,
         xlab = "1 - Specificity in %",
         ylab = "Sensitivity in %")
    plot_initialized <- TRUE
  } else {
    plot(ensemble_roc_subgroup,
         col = age_colors_risk[i], lwd = 4,
         legacy.axes = TRUE,
         add = TRUE)
  }
}

# Create legend labels with AUCs
age_labels_with_auc <- paste(age_group_labels, "(AUC:", age_aucs, ")")

# Add legend
legend("bottomright",
       legend = age_labels_with_auc,
       col = age_colors_risk,
       lwd = 4,
       cex = 0.8,
       bty = "n",
       inset = c(-0.1, 0))

```

### Combined Presentation

```{r}

# Configure a 1x2 plotting layout and increase font size globally
par(mfrow = c(1, 2),
    pty = "s",
    cex = 1.2)

# Gender Group ROC Curves
gender_groups <- c("Female ", "Male")  
gender_aucs <- c()
plot_initialized <- FALSE

for (i in 1:length(gender_groups)) {
  gender_code <- as.character(i - 1)  
  indices <- which(train_data_risk$gender == gender_code)
  EVT_labels_subgroup <- EVT_labels[indices]               
  ensemble_predictions_subgroup <- ensemble_predictions[indices]
  if (length(unique(EVT_labels_subgroup)) < 2) next

  ensemble_roc_subgroup <- roc(EVT_labels_subgroup,
                               ensemble_predictions_subgroup,
                               levels = c(0, 1),
                               direction = "<",
                               percent = TRUE)
  
  gender_aucs[i] <- round(ensemble_roc_subgroup$auc, 0)

  if (!plot_initialized) {
    plot(ensemble_roc_subgroup,
         col = gender_colors_risk[i], lwd = 4,
         legacy.axes = TRUE,
         xlab = "1 - Specificity in %",
         ylab = "Sensitivity in %")
    plot_initialized <- TRUE
  } else {
    plot(ensemble_roc_subgroup,
         col = gender_colors_risk[i], lwd = 4,
         legacy.axes = TRUE, add = TRUE)
  }
}

gender_labels_with_auc <- paste(gender_groups, "( AUC:", gender_aucs, ")")
legend("bottomright", legend = gender_labels_with_auc,
       col = gender_colors_risk,
       lwd = 4,
       seg.len = 0.6,
       x.intersp = 0.2,
       bty = "n",
       inset = c(-0.4, 0))


# Age Group ROC Curves
age_groups <- unique(train_data_risk$age_group)
age_group_labels <- c("    < 60", "60 - 80", "    > 80")
age_aucs <- c()
plot_initialized <- FALSE

for (i in 1:length(age_groups)) {
  age_group <- age_groups[i]
  indices <- which(train_data_risk$age_group == age_group)
  EVT_labels_subgroup <- EVT_labels[indices]                
  ensemble_predictions_subgroup <- ensemble_predictions[indices]
  if (length(unique(EVT_labels_subgroup)) < 2) next

  ensemble_roc_subgroup <- roc(EVT_labels_subgroup, ensemble_predictions_subgroup,
                               levels = c(0, 1), direction = "<", percent = TRUE)
  age_aucs[i] <- round(ensemble_roc_subgroup$auc, 0)

  if (!plot_initialized) {
    plot(ensemble_roc_subgroup,
         col = age_colors_risk[i], lwd = 4,
         legacy.axes = TRUE,
         xlab = "1 - Specificity in %",
         ylab = "Sensitivity in %")
    plot_initialized <- TRUE
  } else {
    plot(ensemble_roc_subgroup,
         col = age_colors_risk[i], lwd = 4,
         legacy.axes = TRUE, add = TRUE)
  }
}

age_labels_with_auc <- paste(age_group_labels, "( AUC:", age_aucs, ")")
legend("bottomright", legend = age_labels_with_auc,
       col = age_colors_risk,
       lwd = 4,
       seg.len = 0.6,
       x.intersp = 0.2,
       bty = "n",
       inset = c(-0.38, 0))

# Reset layout to default
par(mfrow = c(1, 1))

```

# Bootstrapped Performance Distributions

This section compares the bootstrap distributions and confidence intervals of performance metrics across all models using ridgeline plots. We use 5,000 bootstrap resamples with BCa (bias-corrected and accelerated) confidence intervals, consistent with the methodology applied in individual model scripts.

## Data Preparation

We define a custom function to calculate all performance metrics for each bootstrapped dataset. This function will be called by \`boot::boot()\` for each of the 5,000 resamples.

**Choose Model Thresholds from the ML-Scripts**

```{r}

model_thresholds <- c(
  "OneR"  = 0.5,
  "KNN"   = 0.5,
  "CART"  = 0.4,  
  "XGB"   = 0.4,   
  "RF"    = 0.4,   
  "NN"    = 0.4,   
  "4I-SS" = 2/7   # 2/7 equals Score > 2
)

```

**Create a Single Master Table with all Information for the Bootstrapping**

We create a dataframe with the label (ground truth), all observations of the test data, and the predicted probability for each model.

```{r}

# Combines truth and probabilities/scores from all models
data_for_bootstrapping <- tibble(
     EVT     = label_test,
     OneR    = model_scores$OneR,
     CART 
   = model_scores$CART,
     KNN     = model_scores$KNN,
     XGB     = model_scores$XGB,
     RF      = model_scores$RF,
     NN      = model_scores$NN,
     `4I-SS` = model_scores$`4I-SS`
)

# Check the data structure
head(data_for_bootstrapping)

```

## Define Custom Function

We define a custom function to calculate all performance metrics for each bootstrapped dataset. This function will be called by \`boot::boot()\` for each of the 5,000 resamples.

```{r}

calc_boot_metrics_all_models <- function(data, indices) {

  # Resample data for paired bootstrapping
  d <- data[indices, ]
  model_names <- names(model_thresholds)
  results <- c()

  for (model in model_names) {
    probs <- d[[model]]
    truth <- d$EVT
    thresh <- model_thresholds[[model]]

    # Apply model-specific threshold for binary classification
    pred_class <- factor(ifelse(probs > thresh, "1", "0"),
                         levels = c("1", "0"))

    # Calculate threshold-independent metrics
    roc_obj <- pROC::roc(truth,
                         probs,
                         levels = c("0", "1"),
                         direction = "<",
                         quiet = TRUE)
    
    aucroc <- as.numeric(roc_obj$auc)

    pr_obj <- PRROC::pr.curve(scores.class0 = probs[truth == "1"],
                              scores.class1 = probs[truth == "0"],
                              curve = FALSE)
    aucpr <- pr_obj$auc.integral

    # Calculate threshold-dependent metrics
    acc  <- yardstick::accuracy_vec(truth, pred_class)
    sens <- yardstick::sensitivity_vec(truth, pred_class, event_level = "first")
    spec <- yardstick::specificity_vec(truth, pred_class, event_level = "first")
    ppv  <- yardstick::precision_vec(truth, pred_class, event_level = "first")
    npv  <- yardstick::npv_vec(truth, pred_class, event_level = "first")
    f1   <- yardstick::f_meas_vec(truth, pred_class, event_level = "first")

    
    # Add metrics to resulting vector 
    results <- c(results, aucroc, aucpr, acc, sens, spec, f1, ppv, npv)
  }
  return(results)
}

```

We perform the bootstrap simulation with 5,000 replicates.

```{r}

set.seed(456)

# Execute bootstrapping with 5,000 replicates using the statistic function
boot_results_all <- boot::boot(
  data = data_for_bootstrapping,
  statistic = calc_boot_metrics_all_models,
  R = 5000 
)

# Define metrics strictly matching the function output order
calc_order_names <- c("AUCROC",
                      "AUCPR",
                      "Accuracy",
                      "Sensitivity",
                      "Specificity",
                      "F1",
                      "PPV",
                      "NPV")

model_names <- names(model_thresholds) 

# Create column mapping
col_mapping <- expand.grid(Metric = calc_order_names, Model = model_names)

# Convert matrix to a dataframe
boot_samples_df <- as.data.frame(boot_results_all$t)

# Assign column names
colnames(boot_samples_df) <- paste(col_mapping$Model, col_mapping$Metric, sep = "_")

# Define visualization order
desired_plot_order <- c("AUCROC",
                        "AUCPR",
                        "Accuracy",
                        "Sensitivity",
                        "Specificity",
                        "PPV",
                        "NPV",
                        "F1")

# Transform to long format and set factor levels for plotting
boot_results_all_long <- boot_samples_df %>%
  pivot_longer(everything(), names_to = "Model_Metric", values_to = "Value") %>%
  separate(Model_Metric, into = c("Model", "Metric"), sep = "_", extra = "merge") %>%
  mutate(
    Model = factor(Model, levels = model_names),
    Metric = factor(Metric, levels = desired_plot_order) 
  )

```

**Calculate BCa´s Confidence Intervals**

```{r}

# BCa CI helper that returns lower/upper
get_bca_limits <- function(boot_obj, index, conf) {
  ci <- boot::boot.ci(boot_obj, type = "bca", conf = conf, index = index)
  ci$bca[4:5]  # lower, upper
}

get_bca_and_mean <- function(boot_obj, index) {

  # Calculate the bootstrap mean to include it in the plot
  boot_mean <- mean(boot_obj$t[, index], na.rm = TRUE)

  # Calculate uncertainty bands for BCa 95/80/50
  ci95 <- get_bca_limits(boot_obj, index, 0.95)
  ci80 <- get_bca_limits(boot_obj, index, 0.80)
  ci50 <- get_bca_limits(boot_obj, index, 0.50)

  list(
    mean = boot_mean * 100,
    q025 = ci95[1] * 100, q975 = ci95[2] * 100,
    q10  = ci80[1] * 100, q90  = ci80[2] * 100,
    q25  = ci50[1] * 100, q75  = ci50[2] * 100
  )
}

# Build intervals per Model and Metric 
interval_data <- purrr::map_dfr(seq_len(nrow(col_mapping)), function(i) {

  res <- get_bca_and_mean(boot_results_all, index = i)

  tibble::tibble(
    Model  = col_mapping$Model[i],
    Metric = col_mapping$Metric[i],
    mean   = res$mean,
    q025   = res$q025, q975 = res$q975,
    q10    = res$q10,  q90  = res$q90,
    q25    = res$q25,  q75  = res$q75
  )
}) %>%
  dplyr::mutate(
    Model = factor(Model, levels = rev(model_names)),
    Metric = factor(Metric, levels = desired_plot_order),
    mean_label = sprintf("%.0f", mean)
  )

# Prepare long bootstrap data (for densities + axis ranges)
plot_data <- boot_results_all_long %>%
  dplyr::mutate(
    Model  = factor(Model, levels = rev(model_names)),
    Metric = factor(Metric, levels = desired_plot_order),
    perf   = Value * 100
  )

# Per-metric x-range for text and headers
metric_meta <- plot_data %>%
  dplyr::group_by(Metric) %>%
  dplyr::summarise(
    min_val = min(perf, na.rm = TRUE),
    max_val = max(perf, na.rm = TRUE),
    range   = max_val - min_val,
    .groups = "drop"
  )

interval_data <- interval_data %>%
  dplyr::left_join(metric_meta, by = "Metric") %>%
  dplyr::mutate(text_x = min_val - (range * 0.18))

# Headers and smaller titels
n_models <- length(model_names)

header_data <- metric_meta %>%
  dplyr::mutate(
    y_pos  = n_models + 1.0,
    text_x = min_val - (range * 0.18),
    label  = "Mean"
  )

title_data <- metric_meta %>%
  dplyr::mutate(
    y_pos  = n_models + 2.2,
    text_x = min_val - (range * 0.18),
    label  = as.character(Metric)
  )

# Set same colors as model have in the ROC and PR curves
model_colors_base <- setNames(
  substr(colors_nejm_alpha90[seq_along(model_names)], 1, 7),
  model_names
)

# Band widths (95/80/50)
band_lwd_95 <- 2.6
band_lwd_80 <- 3.0
band_lwd_50 <- 3.4

```

## Create `ggdist`-Plots

```{r}

# Define model colors matching the ROC curves
colors_nejm_alpha90 <- c(
       "#BC3C29E6",
       "#E18727E6",
       "#0072B5E6",
       "#20854EE6", 
       "#7876B1E6",
       "#6F99ADE6",
       "#FFDC91E6",
       "#EE4C97E6"
       )

```

**Standardize Text Positions for 0-100 Scale**

We overwrite the text_x coordinates to ensure alignment across all facets on the fixed 0-100 scale.

```{r}

# Position the "Mean" column on the left (e.g., at x = 5%)
interval_data$text_x <- 5 
header_data$text_x   <- 5

# Position the Metric Titles also on the left (e.g., at x = 5%)
title_data$text_x    <- 5

```

**Do the Plot Using `ggplot` and `ggdist`**

```{r}

ridget_plot <- ggplot(plot_data, aes(x = perf, y = Model)) +

  # A. Density
  ggdist::stat_slab(
    fill = "grey85",        
    alpha = 1,
    color = NA,
    adjust = 0.8,
    width = 0.6,            
    justification = -0.2,   
    normalize = "groups"    
  ) +

  # B. Intervals
  geom_linerange(
    data = interval_data,
    aes(y = Model, xmin = q025, xmax = q975, color = Model),
    linewidth = band_lwd_95, alpha = 0.50, inherit.aes = FALSE, show.legend = FALSE
  ) +
  geom_linerange(
    data = interval_data,
    aes(y = Model, xmin = q10, xmax = q90, color = Model),
    linewidth = band_lwd_80, alpha = 0.75, inherit.aes = FALSE, show.legend = FALSE
  ) +
  geom_linerange(
    data = interval_data,
    aes(y = Model, xmin = q25, xmax = q75, color = Model),
    linewidth = band_lwd_50, alpha = 1.00, inherit.aes = FALSE, show.legend = FALSE
  ) +

  # C. Mean Point
  geom_point(
    data = interval_data,
    aes(x = mean, y = Model),
    inherit.aes = FALSE, color = "black", size = 1.5, show.legend = FALSE
  ) +

  # D. Numbers Column 
  geom_text(
    data = interval_data,
    aes(x = text_x, y = Model, label = mean_label),
    inherit.aes = FALSE,
    size = 3.2,
    family = "mono",
    fontface = "bold",
    color = "grey20",
    hjust = 0.5 
  ) +

  # E. Header "Mean" 
  geom_text(
    data = header_data,
    aes(x = text_x, y = y_pos, label = label),
    inherit.aes = FALSE,
    size = 3,
    fontface = "bold",
    color = "grey30",
    hjust = 0.5, 
    vjust = 0 
  ) +

  # F. Metric Title 
  geom_text(
    data = title_data,
    aes(x = text_x, y = y_pos, label = label),
    inherit.aes = FALSE,
    size = 4.5,        # bigger than the rest
    fontface = "bold",
    color = "black",
    hjust = 0.5,       # centered
    vjust = 0
  ) +

  # free_x for 0-100 across all x axis
  facet_wrap(~Metric, scales = "free_x", ncol = 2) +

  # Apply NEJM Colors
  scale_color_manual(values = model_colors_base) +
   
  # Scales from 0 to 100
  scale_x_continuous(
     breaks = seq(0, 100, by = 10),
     expand = expansion(mult = c(0, 0.05)) 
  ) +
   
  scale_y_discrete(
    # For more title space
    expand = expansion(mult = c(0.05, 0.45)) 
  ) +

  coord_cartesian(xlim = c(0, 100), clip = "off") +

  labs(x = "Performance (%)", y = NULL) +

  theme_minimal(base_family = "sans") +
  theme(
    legend.position = "none",
    panel.grid.minor = element_blank(),
    panel.grid.major.y = element_line(linewidth = 0.3, color = "grey92"), 
    panel.grid.major.x = element_line(linewidth = 0.3, color = "grey92"), 
    strip.text = element_blank(), 
    strip.background = element_blank(),
    axis.text.y = element_text(face = "bold", size = 9, color = "black", hjust = 0),
    axis.text.x = element_text(size = 8, color = "grey30"),
    axis.title.x = element_text(size = 10, margin = margin(t = 10)),

    panel.spacing = unit(2.5, "lines"), 
    plot.margin = margin(t = 40, r = 10, b = 10, l = 10)
  )

# Print the plot and supress "Picking joint bandwidth..."
suppressMessages(print(ridget_plot))

```
